{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "import openai\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer,T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer,DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "import random\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00dc75af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perspectrum\n",
      "Query size and length 100 15.01\n",
      "Corpus size and length 500 11.026\n",
      "agnews\n",
      "Query size and length 100 84.39\n",
      "Corpus size and length 500 162.01\n",
      "story\n",
      "Query size and length 100 24.58\n",
      "Corpus size and length 500 15.916\n",
      "ambigqa\n",
      "Query size and length 100 12.32\n",
      "Corpus size and length 500 28.858\n",
      "allsides\n",
      "Query size and length 100 12.21\n",
      "Corpus size and length 500 1075.936\n",
      "exfever\n",
      "Query size and length 100 49.37\n",
      "Corpus size and length 500 28.378\n"
     ]
    }
   ],
   "source": [
    "path = \"./demo_pir_dataset.json\"\n",
    "    \n",
    "with open(path,\"r\",encoding=\"utf-8\") as f:\n",
    "    datasets = json.load(f)\n",
    "\n",
    "for k,v in datasets.items():\n",
    "    print(k)\n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f57abd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_perspectrum\n",
      "16\n",
      "Query size and length 16 7.9375\n",
      "Corpus size and length 500 11.026\n",
      "source_agnews\n",
      "50\n",
      "Query size and length 50 69.86\n",
      "Corpus size and length 500 162.01\n",
      "source_story\n",
      "50\n",
      "Query size and length 50 13.08\n",
      "Corpus size and length 500 15.916\n",
      "source_ambigqa\n",
      "26\n",
      "Query size and length 26 9.23076923076923\n",
      "Corpus size and length 500 28.858\n",
      "source_allsides\n",
      "17\n",
      "Query size and length 17 1.588235294117647\n",
      "Corpus size and length 500 1075.936\n",
      "source_exfever\n",
      "34\n",
      "Query size and length 34 40.529411764705884\n",
      "Corpus size and length 500 28.378\n"
     ]
    }
   ],
   "source": [
    "# create a root query only dataset\n",
    "source_datasets = {}\n",
    "\n",
    "for data_name, dataset in datasets.items():\n",
    "    # {\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"corpus\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    source_datasets[\"source_\"+data_name] = {\"corpus\":dataset[\"corpus\"],\"queries\":[],\"source_queries\":[],\"perspectives\":[],\"key_ref\":{},\"query_labels\":[]}\n",
    "    \n",
    "    reverse_source_query_dic = {}\n",
    "    \n",
    "    for i, query in enumerate(dataset[\"source_queries\"]):\n",
    "        if query not in list(reverse_source_query_dic.keys()):\n",
    "            query_id = str(len(source_datasets[\"source_\"+data_name][\"queries\"]))\n",
    "            reverse_source_query_dic[query] = query_id\n",
    "            source_datasets[\"source_\"+data_name][\"queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"source_queries\"].append(query)\n",
    "            source_datasets[\"source_\"+data_name][\"perspectives\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"query_labels\"].append(\"none\")\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][query_id] = dataset[\"key_ref\"][str(i)]\n",
    "        else:\n",
    "            # this source query already exists\n",
    "            source_datasets[\"source_\"+data_name][\"key_ref\"][str(reverse_source_query_dic[query])].extend(dataset[\"key_ref\"][str(i)])\n",
    "\n",
    "        \n",
    "for k,v in source_datasets.items():\n",
    "    print(k)\n",
    "    print(len(v[\"key_ref\"].keys()))\n",
    "    \n",
    "    q_len = [len(x.split()) for x in v[\"queries\"]]\n",
    "    c_len = [len(x.split()) for x in v[\"corpus\"]]\n",
    "\n",
    "    print(\"Query size and length\",len(v[\"queries\"]),sum(q_len)/len(q_len))\n",
    "    print(\"Corpus size and length\",len(v[\"corpus\"]),sum(c_len)/len(c_len))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199f9893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(key_ref, corpus_scores, query_labels, dataset_name):\n",
    "    # evaluation of a dataset    \n",
    "    recall_threshold = [1,5,10]\n",
    "    recall_results = [0 for thresh in recall_threshold]\n",
    "    \n",
    "    if \"source\" in dataset_name:\n",
    "        parts = [\"none\"]\n",
    "    else:\n",
    "        if dataset_name == \"perspectrum\":\n",
    "            parts = [\"support\",\"undermine\",\"general\"]\n",
    "        elif dataset_name == \"agnews\":\n",
    "            parts = [\"subtopic\", \"location\"]\n",
    "        elif dataset_name == \"story\":\n",
    "            parts = [\"analogy\", \"entity\"]\n",
    "        elif dataset_name == \"ambigqa\":\n",
    "            parts = [\"perspective\"]\n",
    "        elif dataset_name == \"allsides\":\n",
    "            parts = [\"left\",\"right\",\"center\"]\n",
    "        elif dataset_name == \"exfever\":\n",
    "            parts = [\"SUPPORT\",\"REFUTE\",\"NOT ENOUGH INFO\"]\n",
    "    \n",
    "    parts_size = [0 for x in parts]\n",
    "        \n",
    "    for lb in query_labels:\n",
    "        parts_size[parts.index(lb)] += 1\n",
    "            \n",
    "    partial_recall_results = []\n",
    "    for i in range(len(parts)):\n",
    "        partial_recall_results.append([0 for thresh in recall_threshold])\n",
    "\n",
    "    \n",
    "    for k,v in key_ref.items():\n",
    "        for j, thresh in enumerate(recall_threshold):\n",
    "            # important: find one is ok, this can be modified\n",
    "            ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "            \n",
    "            \n",
    "            indicator = 0\n",
    "            try:\n",
    "                for index in v:\n",
    "                    if index in ranked_scores:\n",
    "                        indicator = 1 \n",
    "            except:\n",
    "                for index in [v]:\n",
    "                    if index in ranked_scores:\n",
    "                        indicator = 1                \n",
    "            recall_results[j] += indicator\n",
    "            partial_recall_results[parts.index(query_labels[int(k)])][j] += indicator\n",
    "    \n",
    "    final_results = [result/len(key_ref.items()) for result in recall_results]\n",
    "        \n",
    "    print(\"overall\")\n",
    "    for i, thresh in enumerate(recall_threshold):\n",
    "        print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "        \n",
    "    macro_threshs = [[] for x in recall_threshold]\n",
    "    \n",
    "    for t, recall_results in enumerate(partial_recall_results):\n",
    "        print(parts[t])\n",
    "        final_results = [result/parts_size[t] for result in recall_results]\n",
    "        \n",
    "        for i, thresh in enumerate(recall_threshold):\n",
    "            print(\"Recall@\"+str(thresh)+\":\",final_results[i])\n",
    "            macro_threshs[i].append(final_results[i])\n",
    "                \n",
    "    print(\"macro_average\")\n",
    "    for i, thresh in enumerate(recall_threshold):\n",
    "        print(\"Recall@\"+str(thresh)+\":\",sum(macro_threshs[i])/len(macro_threshs[i]))\n",
    "                \n",
    "                    \n",
    "\n",
    "# BM25 and BERTScore\n",
    "from rank_bm25 import BM25Okapi\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def bm25_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        \n",
    "        tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        corpus_scores = []\n",
    "\n",
    "        for query in tqdm(queries):\n",
    "            # query = item[\"query\"]\n",
    "            tokenized_query = query.split(\" \")\n",
    "            doc_scores = bm25.get_scores(tokenized_query)\n",
    "            corpus_scores.append(doc_scores)\n",
    "        \n",
    "        with open(\"bm25_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump([x.tolist() for x in corpus_scores],f)\n",
    "        \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def extract_layer_cls(embeddings,layer):\n",
    "    rep = []\n",
    "    this_layer_embeddings = embeddings[layer]\n",
    "    for emb in this_layer_embeddings:\n",
    "        rep.append(emb[0])\n",
    "\n",
    "\n",
    "    return rep\n",
    "\n",
    "        \n",
    "def create_embeddings(tokenizer, model, texts):\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    # create tokenized inputs\n",
    "    batch_size = 17 #29\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # naive batching\n",
    "    if len(texts) < batch_size:\n",
    "        inputs = tokenizer(texts,max_length=80, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "            embeddings = []\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        embeddings = []\n",
    "        num_batch = len(texts)//batch_size\n",
    "\n",
    "        for i in trange(num_batch+1):\n",
    "            batch_start = i*batch_size\n",
    "            batch_end = min(len(texts), (i+1)*batch_size)\n",
    "            batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                    embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "\n",
    "                    # save cuda memory\n",
    "                    del batch_embeddings\n",
    "                    del inputs\n",
    "                    torch.cuda.empty_cache()\n",
    "                except:\n",
    "                    message = \"broken embeddings\"\n",
    "\n",
    "    # 25 * num_example * seq_len * 768 -> num_example * 768\n",
    "    return embeddings\n",
    "        \n",
    "    \n",
    "def dpr_main(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    elif model_name in [\"simcse-unsup\",\"simcse-sup\"]:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "            \n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "            query_embeddings = create_T5_embeddings(qtokenizer, qmodel, queries,0)\n",
    "            corpus_embeddings = create_T5_embeddings(ctokenizer, cmodel, corpus, 0)  \n",
    "        else:\n",
    "            query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "            corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(model_name+\"_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "        \n",
    "def contriever_main(datasets):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "    \n",
    "    def mean_pooling(token_embeddings, mask):\n",
    "        token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "        sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "        \n",
    "        return sentence_embeddings\n",
    "    \n",
    "    def contriever_embeddings(texts, tokenizer, model):\n",
    "        # device = torch.device('cuda')\n",
    "        device = torch.device('cpu')\n",
    "        # create tokenized inputs\n",
    "        batch_size = 29\n",
    "\n",
    "        model.to(device)\n",
    "        embeddings = []\n",
    "        # naive batching\n",
    "        if len(texts) < batch_size:\n",
    "            inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "            outputs = model(**inputs)    \n",
    "            batch_embeddings = mean_pooling(outputs[0], inputs['attention_mask'])\n",
    "            for embedding in batch_embeddings:\n",
    "                embeddings.append(embedding.detach().cpu().tolist())\n",
    "                \n",
    "            del batch_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            num_batch = len(texts)//batch_size\n",
    "\n",
    "            for i in trange(num_batch+1):\n",
    "                batch_start = i*batch_size\n",
    "                batch_end = min(len(texts), (i+1)*batch_size)\n",
    "                batch_texts = texts[batch_start:batch_end]\n",
    "\n",
    "                inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    try:\n",
    "                        batch_embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output\n",
    "                        embeddings.extend(batch_embeddings.detach().cpu().tolist())\n",
    "                        del batch_embeddings\n",
    "                        del inputs\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except:\n",
    "                        message = \"broken embeddings\"   \n",
    "                        \n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
    "#     model = AutoModel.from_pretrained('facebook/contriever-msmarco')\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('facebook/contriever')\n",
    "    model = AutoModel.from_pretrained('facebook/contriever')\n",
    "    \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "            \n",
    "        query_embeddings = contriever_embeddings(queries, tokenizer, model)\n",
    "        corpus_embeddings = contriever_embeddings(corpus, tokenizer, model)\n",
    "        \n",
    "        for emb1 in tqdm(query_embeddings):\n",
    "            scores = []\n",
    "            for emb2 in corpus_embeddings:\n",
    "                scores.append(1 - cosine(emb1, emb2))\n",
    "\n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"contriver_\"+k+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "# running the processing on source datasets by changing the names of the datasets\n",
    "        \n",
    "# print(\"BM25\")\n",
    "# bm25_main(datasets)\n",
    "\n",
    "# model_names = [\"dpr\", \"simcse-unsup\", \"simcse-sup\",\"abs\",\"aspire\"]\n",
    "# for model_name in model_names:\n",
    "#     print(\"============\", model_name, \"============\")\n",
    "#     dpr_main(datasets, model_name) \n",
    "\n",
    "# print(\"Contriever\")\n",
    "# contriever_main(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c8338",
   "metadata": {},
   "source": [
    "# Results Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_score_collection = {}\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    corpus_score_collection[retriever] = {}\n",
    "    for data_name in ['agnews', 'perspectrum', 'story','allsides','exfever','ambigqa']:\n",
    "        try:\n",
    "            if \"tart\" not in retriever:\n",
    "                with open(\"./scores/\"+retriever+\"_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)\n",
    "            else:\n",
    "                with open(\"./scores/\"+retriever+data_name+\"_scores.json\",\"r\") as f:\n",
    "                    corpus_score_collection[retriever][data_name] = json.load(f)                \n",
    "        except:\n",
    "            print(retriever,data_name)\n",
    "            \n",
    "        try:\n",
    "            with open(\"./scores/\"+retriever+\"_source_\"+data_name+\"_scores.json\",\"r\") as f:\n",
    "                corpus_score_collection[retriever][\"source_\"+data_name] = json.load(f)\n",
    "        except:\n",
    "            print(\"source\",retriever,data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d3564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "def evaluation_for_writing_mrr(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "    \n",
    "    mrrs = []\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        v = key_ref[str(k)]\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()\n",
    "        ranked_scores = ranked_scores.tolist()\n",
    "        rr = []\n",
    "        for one_correct_doc in v:\n",
    "            rr.append(1/(ranked_scores.index(one_correct_doc)+1))\n",
    "        \n",
    "        if len(rr) == 0:\n",
    "            mrrs.append(0)\n",
    "        else:\n",
    "            mrrs.append(sum(rr)/len(rr))\n",
    "            \n",
    "    return mrrs \n",
    "\n",
    "def evaluation_for_writing_recalls(datasets, corpus_scores, dataset_name, thresh):\n",
    "    # evaluation of a dataset    \n",
    "    recall_results = []\n",
    "    \n",
    "    key_ref = datasets[dataset_name][\"key_ref\"]\n",
    "    query_labels = datasets[dataset_name][\"query_labels\"]\n",
    "\n",
    "    for k in range(len(corpus_scores)):\n",
    "        \n",
    "        v = key_ref[str(k)]\n",
    "\n",
    "        ranked_scores = (-np.array(corpus_scores[int(k)])).argsort()[:thresh]\n",
    "        indicator = 0\n",
    "        try:\n",
    "            for index in v:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1 \n",
    "        except:\n",
    "            for index in [v]:\n",
    "                if index in ranked_scores:\n",
    "                    indicator = 1     \n",
    "                    \n",
    "        recall_results.append(indicator)\n",
    "    \n",
    "    return recall_results\n",
    "\n",
    "\n",
    "mrr_collection = {}\n",
    "\n",
    "# prepare to compute perspective-aware scores: \n",
    "a mapping to group queries with the same root query {data_name: [[0,1,2],[3,4]]}\n",
    "root_mapping = {}\n",
    "\n",
    "for data_name in ['agnews','perspectrum', 'story','allsides','ambigqa',\"exfever\"]: \n",
    "    this_source_qs = source_datasets[\"source_\"+data_name][\"queries\"]\n",
    "    root_mapping[data_name] = [[] for x in this_source_qs]\n",
    "    \n",
    "    for i,query in enumerate(datasets[data_name][\"queries\"]):\n",
    "        sq = datasets[data_name][\"source_queries\"][i]\n",
    "        root_mapping[data_name][this_source_qs.index(sq)].append(i)\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\"]:\n",
    "    mrr_collection[retriever] = {}\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        per_corpus_score = corpus_score_collection[retriever][data_name]        \n",
    "        recalls = evaluation_for_writing_recalls(datasets, per_corpus_score, data_name,5)\n",
    "        \n",
    "        p_mrrs = []\n",
    "        for lst in root_mapping[data_name]:\n",
    "            temp = []\n",
    "            for x in lst:\n",
    "                if x < len(recalls):\n",
    "                    temp.append(recalls[x])\n",
    "            p_mrrs.append(temp)\n",
    "        \n",
    "        p_vars = []\n",
    "        for x in p_mrrs:\n",
    "            if len(x) > 1:\n",
    "                p_vars.append(statistics.mean(x))\n",
    "            elif len(x) == 1:\n",
    "                p_vars.append(0.0) #x[0]\n",
    "            else:\n",
    "                # equal 0\n",
    "                continue\n",
    "        \n",
    "        mrr_collection[retriever][data_name] = sum(p_vars)/len(p_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2 in the draft\n",
    "\n",
    "for retriever in [\"bm25\",\"dpr\",\"simcse-sup\",\"simcse-unsup\",\"contriver\",\"tart\"]:\n",
    "    print(r_name_map[retriever],end=\" \")\n",
    "    temp_str = \"\"\n",
    "    temp_score = []\n",
    "    for data_name in ['agnews','story','perspectrum','ambigqa','allsides','exfever']: \n",
    "        temp_str += \"&\"+ str(round(mrr_collection[retriever][data_name]*100,1)) + \" \"\n",
    "        temp_score.append(round(mrr_collection[retriever][data_name]*100,1))\n",
    "        \n",
    "    mean_score = sum(temp_score)/len(temp_score)\n",
    "    print(temp_str+ \"&\" + str(round(mean_score,1))+ \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sec 4.2 Exploring PIR\n",
    "# collection of embeddings to enable future use\n",
    "\n",
    "def extract_embeddings(datasets, model_name):\n",
    "    # corpuses,key_refs = corpus_building(datasets)\n",
    "\n",
    "    if model_name == \"dpr\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        \n",
    "    if model_name == \"dpr-multiset\":\n",
    "        ctokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        cmodel = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        qtokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        qmodel = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        \n",
    "    elif \"simcse\" in model_name:\n",
    "        model_mapping = {\n",
    "            \"simcse-unsup\":\"princeton-nlp/unsup-simcse-bert-base-uncased\",\n",
    "            \"simcse-sup\":\"princeton-nlp/sup-simcse-bert-base-uncased\",\n",
    "            \"simcse-unsup-large\":\"princeton-nlp/unsup-simcse-bert-large-uncased\",\n",
    "            \"simcse-sup-large\":\"princeton-nlp/sup-simcse-bert-large-uncased\",\n",
    "            \"simcse-unsup-rb\":\"princeton-nlp/unsup-simcse-roberta-base\",\n",
    "            \"simcse-sup-rb\":\"princeton-nlp/sup-simcse-roberta-base\",\n",
    "            \"simcse-unsup-large-rb\":\"princeton-nlp/unsup-simcse-roberta-large\",\n",
    "            \"simcse-sup-large-rb\":\"princeton-nlp/sup-simcse-roberta-large\"\n",
    "        }\n",
    "        \n",
    "        ctokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        cmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "        qtokenizer = AutoTokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = AutoModel.from_pretrained(model_mapping[model_name])\n",
    "\n",
    "        \n",
    "        \n",
    "    if model_name in [\"t5\",\"flan-t5\",\"unifiedqa\"]:\n",
    "        # encoder vs. decoder\n",
    "        model_mapping = {\n",
    "            \"t5\":\"google-t5/t5-large\",\n",
    "            \"flan-t5\": \"google/flan-t5-large\",\n",
    "            \"unifiedqa\": \"allenai/unifiedqa-v2-t5-large-1251000\"\n",
    "        }\n",
    "        \n",
    "        qtokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        ctokenizer = T5Tokenizer.from_pretrained(model_mapping[model_name])\n",
    "        qmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).decoder #decoder\n",
    "        cmodel = T5ForConditionalGeneration.from_pretrained(model_mapping[model_name]).encoder #decoder\n",
    "\n",
    "    qmodel.eval() \n",
    "    cmodel.eval() \n",
    "    \n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        corpus_scores = []\n",
    "        \n",
    "        queries = v[\"queries\"]\n",
    "        source_queries = v[\"source_queries\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "     \n",
    "        query_embeddings = create_embeddings(qtokenizer, qmodel, queries)\n",
    "        source_query_embeddings = create_embeddings(qtokenizer, qmodel, source_queries)\n",
    "        perspectives_embeddings = create_embeddings(qtokenizer, qmodel, perspectives)\n",
    "        corpus_embeddings = create_embeddings(ctokenizer, cmodel, corpus)\n",
    "\n",
    "        names = [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]\n",
    "        embs = [query_embeddings, source_query_embeddings, perspectives_embeddings, corpus_embeddings]\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            path = \"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\"\n",
    "            \n",
    "            with open(path,\"w\",encoding=\"utf-8\") as f:\n",
    "                json.dump(embs[i],f)\n",
    "    \n",
    "model_lists = [\"dpr\",\"dpr-multiset\"]\n",
    "model_lists.extend([\"simcse-unsup\",\"simcse-sup\",\"simcse-unsup-large\",\"simcse-sup-large\",\"simcse-unsup-rb\",\"simcse-sup-rb\",\"simcse-unsup-large-rb\",\"simcse-sup-large-rb\"])\n",
    "\n",
    "\n",
    "# for model_name in model_lists:\n",
    "#     print(model_name)\n",
    "#     extract_embeddings(datasets, model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d78f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation below is for demo-purpose\n",
    "\n",
    "def general_pir_main(datasets, model_name=\"simcse-sup\",mode=\"vec_cast\"):\n",
    "\n",
    "    for k,v in datasets.items():\n",
    "        \n",
    "        print(\"we are working on:\",k)\n",
    "        \n",
    "        corpus_scores = []\n",
    "\n",
    "        queries = v[\"queries\"]\n",
    "        query_labels = v[\"query_labels\"]\n",
    "        corpus = v[\"corpus\"]\n",
    "        key_ref = v[\"key_ref\"]\n",
    "        perspectives = v[\"perspectives\"]\n",
    "        \n",
    "        if mode == \"bm25_ranking\":\n",
    "            tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "            bm25 = BM25Okapi(tokenized_corpus) \n",
    "        \n",
    "        embs_collection = {}\n",
    "        \n",
    "        for name in [\"queries\",\"source_queries\",\"perspectives\",\"corpus\"]:\n",
    "            with open(\"./embs/\"+k+\"_\"+model_name+\"_\"+name+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "                embs_collection[name] = json.load(f)\n",
    "            \n",
    "        query_embeddings = embs_collection[\"queries\"]\n",
    "        source_query_embeddings = embs_collection[\"source_queries\"]\n",
    "        perspectives_embeddings = embs_collection[\"perspectives\"]\n",
    "        corpus_embeddings = embs_collection[\"corpus\"]\n",
    "        \n",
    "\n",
    "        for index in trange(len(query_embeddings)):\n",
    "            emb_q = query_embeddings[index]\n",
    "            emb_s = source_query_embeddings[index]\n",
    "            emb_p = perspectives_embeddings[index]\n",
    "            \n",
    "            scores = []\n",
    "            bm25_scores = []\n",
    "            p_scores = []\n",
    "            \n",
    "            for i, emb_c in enumerate(corpus_embeddings):\n",
    "                \n",
    "                # vector manipulation: aug denotes using q, instead of s\n",
    "                if mode == \"vec_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q + weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection_rev\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q + weight_q*emb_p, emb_c + weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    weight = np.dot(emb_q, emb_p)/np.dot(emb_p, emb_p)\n",
    "                    context_score = 1 - cosine(emb_q - weight*emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_projection\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    p_cor = np.dot(emb_p, emb_p)\n",
    "                    weight_q = np.dot(emb_q, emb_p)/p_cor\n",
    "                    weight_c = np.dot(emb_c, emb_p)/p_cor\n",
    "                    context_score = 1 - cosine(emb_q - weight_q*emb_p, emb_c- weight_c*emb_p)\n",
    "                    scores.append(context_score)\n",
    "                \n",
    "                if mode == \"vec_add\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                if mode == \"vec_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_add\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_q+emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_concat\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_qp = np.concatenate((emb_q, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_qp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_dual_concat\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    emb_sp = np.concatenate((emb_s, emb_p), axis=None)\n",
    "                    emb_cp = np.concatenate((emb_c, emb_p), axis=None)\n",
    "                    context_score = 1 - cosine(emb_sp, emb_cp)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_cast_single\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast_single\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c)\n",
    "                    scores.append(context_score)     \n",
    "                    \n",
    "                if mode == \"vec_cast\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                if mode == \"vec_aug_cast\":\n",
    "                    emb_q,emb_c,emb_p = np.array(emb_q),np.array(emb_c),np.array(emb_p)\n",
    "                    if k == \"ambigqa\":\n",
    "                        emb_p = emb_q - emb_s\n",
    "                    context_score = 1 - cosine(emb_q-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "                    \n",
    "                    \n",
    "                # score manipulation\n",
    "                if mode == \"additive\":\n",
    "                    context_score = 1 - cosine(emb_s, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_aug\":\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(context_score + perspective_score)\n",
    "                    \n",
    "                if mode == \"additive_tripple\":\n",
    "                    source_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_q, emb_c)\n",
    "                    perspective_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(source_score + context_score + perspective_score)\n",
    "                    \n",
    "                \n",
    "                # re-ranking\n",
    "                if mode == \"re-ranking\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_s, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "\n",
    "                if mode == \"re-ranking_aug\":\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    q_score = 1 - cosine(emb_q, emb_c)\n",
    "                    context_score = 1 - cosine(emb_p, emb_c)\n",
    "                    scores.append(q_score)\n",
    "                    p_scores.append(context_score)\n",
    "                \n",
    "                if mode == \"bm25_ranking\":\n",
    "                    query = queries[index]\n",
    "                    context = corpus[index]\n",
    "                    kws = perspectives[index].split(\" \")\n",
    "\n",
    "                    emb_s,emb_c,emb_p = np.array(emb_s),np.array(emb_c),np.array(emb_p)\n",
    "                    context_score = 1 - cosine(emb_s-emb_p, emb_c-emb_p)\n",
    "                    scores.append(context_score)\n",
    "\n",
    "                    tokenized_query = query.split(\" \")\n",
    "                    doc_scores = bm25.get_scores(tokenized_query)\n",
    "                    bm25_scores.append(doc_scores[i])\n",
    "            \n",
    "            if mode == \"re-ranking\" or mode == \"re-ranking_aug\":\n",
    "                thresh = float(np.percentile(np.array(scores), 80))\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    if score >= thresh:\n",
    "                        temp_score = score + p_scores[i]\n",
    "                    else:\n",
    "                        temp_score = -100\n",
    "                scores = temp_scores\n",
    "            \n",
    "            if mode == \"bm25_ranking\":\n",
    "                sm = statistics.mean(scores)\n",
    "                bm25m = statistics.mean(bm25_scores)\n",
    "                temp_scores = []\n",
    "                for i, score in enumerate(scores):\n",
    "                    temp_scores = score-sm+bm25_scores[i]-bm25m\n",
    "                scores = temp_scores\n",
    "                    \n",
    "            corpus_scores.append(scores)\n",
    "            \n",
    "        with open(\"./pir_scores/pir_\"+k+\"_\"+mode+\"_scores.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(corpus_scores,f)\n",
    "            \n",
    "        evaluation(key_ref, corpus_scores, query_labels, k)\n",
    "        print()\n",
    "\n",
    "all_pir_mode = []\n",
    "\n",
    "# all_pir_mode.extend([\"vec_add\",\"vec_concat\",\"vec_aug_add\",\"vec_aug_concat\",\"vec_dual_concat\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "# all_pir_mode.extend([\"additive\",\"additive_aug\",\"additive_tripple\"])\n",
    "# all_pir_mode.extend([\"vec_cast_single\",\"vec_aug_cast_single\", \"re-ranking\", \"re-ranking_aug\",\"bm25_ranking\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection\",\"vec_dual_projection\",\"vec_cast\",\"vec_aug_cast\"])\n",
    "\n",
    "# all_pir_mode.extend([\"vec_projection_rev\",\"vec_dual_projection_rev\"])\n",
    "# for mode in all_pir_mode:\n",
    "#     print(\"-------------------\",mode,\"--------------------------\")\n",
    "#     general_pir_main(datasets,model_name=\"simcse-sup\",mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the PIR scores computed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
